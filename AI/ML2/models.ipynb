{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for grafic\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    TN = np.logical_not(Y_val | Y_pred).sum()\n",
    "    return (TP + TN) / len(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    return TP / Y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    return TP / Y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_metric(Y_val, Y_pred):\n",
    "    precision = Precision(Y_val, Y_pred)\n",
    "    recall = Recall(Y_val, Y_pred)\n",
    "    return 2.0 * recall * precision / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_norm(vector):\n",
    "    return (vector**2).sum()\n",
    "\n",
    "def L1_norm(vector):\n",
    "    return np.abs(vec).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_grad(vector):\n",
    "    return vector\n",
    "\n",
    "def L1_grad(vector):\n",
    "    return vector / np.abs(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, speed, gradient_func, regulasator=None, \n",
    "                 C=10.0, eps = 0.001, maxsteps=250):\n",
    "        self.speed = speed\n",
    "        self.function = gradient_func\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "        if regulasator == \"l1\":\n",
    "            self.regulasator = lambda w:  L1_grad(w) / C\n",
    "        elif regulasator == \"l2\":\n",
    "            self.regulasator = lambda w: L2_grad(w) / C\n",
    "        else:\n",
    "            self.regulasator = lambda w: 0.0\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # init w0\n",
    "        w0 = np.zeros(X_train.shape[1])\n",
    "        w = np.random.random(X_train.shape[1])\n",
    "        k = 1\n",
    "        while np.linalg.norm(w - w0) > self.eps and k <= self.maxsteps:\n",
    "            w0 = w\n",
    "            temp = self.speed * ((1 / k)**0.5) # like vowpal step temp\n",
    "            w = w - temp*(self.function(X_train, Y_train, w) + self.regulasator(w))\n",
    "            k += 1\n",
    "            \n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def logit_loss(wx, y_real):\n",
    "    return np.log(1.0 + np.exp(-wx*y_real)).sum()\n",
    "\n",
    "def logit_grad(x, y, w):\n",
    "    koeff = (y * sigmoid(-y*x.dot(w)))\n",
    "    koeff = koeff.reshape((koeff.shape[0], 1)) # make a column\n",
    "    return -(koeff * x).sum(axis = 0) # full gradient - sum of gradients on ever x[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    # main params\n",
    "    def __init__(self, speed = 1.5, reg_type=None, C=2.0, eps=0.001, maxsteps=200):\n",
    "        # init solver\n",
    "        self.solver = GradientDescent(speed, logit_grad, reg_type, C, eps, maxsteps)\n",
    "        # init weight  variable\n",
    "        self.w = None\n",
    "        \n",
    "    # training\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # convert 0 to -1 for algo\n",
    "        Y = np.array(Y_train)\n",
    "        Y[Y_train == 0] = -1\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_train.shape[0], 1))\n",
    "        X = np.hstack((x0, X_train))\n",
    "        # train weight by gradient descent\n",
    "        self.w = self.solver.fit(X, Y)\n",
    "        return self\n",
    "    \n",
    "    # returns predictes classes\n",
    "    def predict(self, X_val, border = 0):\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_val.shape[0], 1))\n",
    "        X = np.hstack((x0, X_val))\n",
    "        # <w, x> product for all examples\n",
    "        Xw = X.dot(self.w)\n",
    "        # make predict: 0 - negative, 1 - positive\n",
    "        Y_pred = np.zeros(Xw.shape).astype(np.int8)\n",
    "        # a(x) = [<w,x> > t], t - border\n",
    "        Y_pred[Xw >= border] = 1\n",
    "        return Y_pred\n",
    "    \n",
    "    # probs of positive class\n",
    "    def predict_proba(self, X_val):\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_val.shape[0], 1))\n",
    "        X = np.hstack((x0, X_val))\n",
    "        # <w, x> product for all examples\n",
    "        Xw = X.dot(self.w)\n",
    "        # return proba\n",
    "        return sigmoid(Xw)\n",
    "    \n",
    "    # compute metrics\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.w\n",
    "\n",
    "    # for fun\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression model with gradient descent!\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Logistic Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequental Minimal Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_SMO:\n",
    "    def __init__(self, X, Y, kernel, C, eps, maxsteps, linear):\n",
    "        self.K = kernel\n",
    "        self.C = C\n",
    "        self.tol = eps\n",
    "        self.maxsteps = maxsteps\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.linear = linear\n",
    "        self.l = np.zeros(Y.shape)\n",
    "        self.e_cache = -Y.astype(np.float64) # zero prediction - y\n",
    "        #self.n = len(X)\n",
    "        self.w0 = 0.0\n",
    "        if linear:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "\n",
    "\n",
    "    def KKT_violated(self, idx):\n",
    "        r =  self.Y[idx] * self.e_cache[idx]\n",
    "        l = self.l[idx]\n",
    "        # if (M < 1 - tol & l < C) || (M > 1 + tol & l > 0) => KKT violated\n",
    "        # tol - accuracy of computing\n",
    "        return (l < self.C and r < -self.tol) or (l > 0.0 and r > self.tol)\n",
    "\n",
    "    # search by all element ones\n",
    "    def first_heuristic_one(self):\n",
    "        for idx in range(self.l.shape[0]):\n",
    "            if self.KKT_violated(idx):\n",
    "                yield idx\n",
    "\n",
    "    # search by all non-bound elements \n",
    "    def first_heuristic_two(self):\n",
    "        # i think here loop faster then pre-choice by numpy\n",
    "        # because this loop more effective\n",
    "        for idx in range(self.l.shape[0]):\n",
    "            if 0.0 < self.l[idx] < self.C:\n",
    "                if self.KKT_violated(idx):\n",
    "                    yield idx\n",
    "\n",
    "    # search elem that maximaze |E1 - E2| from non-boundary\n",
    "    def second_heuristic_one(self, idx):\n",
    "        # search |E1 - E2| of each elem\n",
    "        dE = np.abs(self.e_cache - self.e_cache[idx])\n",
    "        # all boundary elems not interesting\n",
    "        dE[(self.l >= self.C) | (self.l <= 0.0)] = 0.0\n",
    "        # return i2 = argmax|E1 - E2|\n",
    "        return np.argmax(dE)\n",
    "\n",
    "    def second_heuristic_two(self, idx):\n",
    "        mask = (self.l < self.C) & (self.l > 0.0)\n",
    "        mask[idx] = False\n",
    "        idxes = np.nonzero(mask)[0]\n",
    "        order = np.random.permutation(len(idxes))\n",
    "        return idxes[order]\n",
    "\n",
    "\n",
    "    def second_heuristic_three(self, idx):\n",
    "        #if second heuristic without result, get idxs without elems from second\n",
    "        # it will faster\n",
    "        mask = (self.l >= self.C) | (self.l <= 0.0)\n",
    "        mask[idx] = False\n",
    "        idxes = np.nonzero(mask)[0]\n",
    "        order = np.random.permutation(len(idxes))\n",
    "        return idxes[order]\n",
    "\n",
    "    def get_weights(self):\n",
    "        if self.linear:\n",
    "            return self.w\n",
    "\n",
    "    def get_support(self):\n",
    "        # returns only support vctors with params if you \n",
    "        # dont get them after train\n",
    "        mask = self.l > 0.0\n",
    "        return self.X[mask], self.Y[mask], self.l[mask], self.w0\n",
    "\n",
    "    # return lambdas\n",
    "    def get_coeffs(self):\n",
    "        return self.l\n",
    "\n",
    "    \n",
    "    def optimize_two(self, i1, i2):\n",
    "        # it emulates machine e in computations\n",
    "        eps = 0.00000000000001\n",
    "\n",
    "        y1 = self.Y[i1]\n",
    "        y2 = self.Y[i2]\n",
    "        x1 = self.X[i1]\n",
    "        x2 = self.X[i2]\n",
    "        l1 = self.l[i1]\n",
    "        l2 = self.l[i2]\n",
    "        E1 = self.e_cache[i1]\n",
    "        E2 = self.e_cache[i2]\n",
    "\n",
    "        # compute L H\n",
    "        if y1 == y2:\n",
    "            L = max(0.0, l2 + l1 - self.C)\n",
    "            H = min(self.C, l2 + l1)\n",
    "        else:\n",
    "            L = max(0.0, l2 - l1)\n",
    "            H = min(self.C, self.C + l2 - l1)\n",
    "        if L == H:\n",
    "            return False\n",
    "\n",
    "        # eta\n",
    "        nu = self.K(x1, x1) + self.K(x2, x2) - 2.0*self.K(x1, x2)\n",
    "\n",
    "        # compute l2\n",
    "        if nu > 0.0:\n",
    "            l2 += y2 * (E1 - E2) / nu\n",
    "            if l2 < L:\n",
    "                l2 = L\n",
    "            elif l2 > H:\n",
    "                l2 = H\n",
    "        else:\n",
    "            c1 = nu/2\n",
    "            c2 = y2 * (E1 - E2) + nu * l2\n",
    "            Lobj = c2*L - c1*L*L\n",
    "            Hobj = c2*H - c1*H*H\n",
    "            if Lobj > Hobj + eps:\n",
    "                l2 = L\n",
    "            elif Lobj < Hobj - eps:\n",
    "                l2 = H\n",
    "\n",
    "        if np.abs(l2 - self.l[i2]) < eps*(l2 + self.l[i2] + eps):\n",
    "            return False\n",
    "\n",
    "        # compute l1\n",
    "        l1 -= y1*y2*(l2 - self.l[i2])\n",
    "        \n",
    "        if l1 < 0.0:\n",
    "            l2 += y1*y2*l1\n",
    "            l1 = 0.0\n",
    "        elif l1 > self.C:\n",
    "            l2 += y1*y2*(l1 - self.C)\n",
    "            l1 = self.C\n",
    "\n",
    "        # update w0:\n",
    "        b1 = self.w0 + E1 + y1*(l1 - self.l[i1])*self.K(x1, x1) + y2*(l2 - self.l[i2])*self.K(x1, x2)\n",
    "        b2 = self.w0 + E2 + y1*(l1 - self.l[i1])*self.K(x1, x2) + y2*(l2 - self.l[i2])*self.K(x2, x2)\n",
    "        if l1 > 0.0 and l1 < self.C:\n",
    "            bnew = b1\n",
    "        elif l2 > 0.0 and l2 < self.C:\n",
    "            bnew = b2\n",
    "        else:\n",
    "            bnew = (b1 + b2) / 2.0\n",
    "        \n",
    "        dw0 = bnew - self.w0\n",
    "        self.w0 = bnew\n",
    "\n",
    "        # update E_cache\n",
    "        t1 = y1*(l1 - self.l[i1])\n",
    "        t2 = y2*(l2 - self.l[i2])\n",
    "        \n",
    "        self.e_cache += t1*self.K(self.X, x1) + t2*self.K(self.X, x2) - dw0\n",
    "        \n",
    "        #update w:\n",
    "        if self.linear:\n",
    "            self.w += t1*x1 + t2*x2\n",
    "        # update lambdas:\n",
    "        self.l[i1] = l1\n",
    "        self.l[i2] = l2\n",
    "\n",
    "        return True\n",
    "            \n",
    "            \n",
    "\n",
    "    # search 2 lambdas for optimize and change them\n",
    "    def train(self):\n",
    "        steps = 0\n",
    "        non_bound_loop = True\n",
    "        # main loop\n",
    "        while steps < self.maxsteps:\n",
    "            non_bound_loop ^= True\n",
    "            # outer loops searches i1:\n",
    "            if not non_bound_loop:\n",
    "                # h1-1\n",
    "                changed = False\n",
    "                for idx1 in self.first_heuristic_one():\n",
    "                    # h2-1\n",
    "                    idx2 = self.second_heuristic_one(idx1)\n",
    "                    if self.optimize_two(idx1, idx2):\n",
    "                        changed = True\n",
    "                        continue\n",
    "                    # h2-2,3 together if h2-1 not worked\n",
    "                    # concat idxs of heuristics in sequence\n",
    "                    extra_heuristics = np.concatenate([\n",
    "                        self.second_heuristic_two(idx1),\n",
    "                        self.second_heuristic_three(idx1)\n",
    "                    ])\n",
    "\n",
    "                    for idx2 in extra_heuristics:\n",
    "                        if self.optimize_two(idx1, idx2):\n",
    "                            changed = True\n",
    "                            break\n",
    "                steps += 1\n",
    "                # if nothing changed - work done\n",
    "                if not changed:\n",
    "                    break\n",
    "            else:\n",
    "                # h1-2\n",
    "                # while changing itterate non-bound elements\n",
    "                while changed and steps < self.maxsteps:\n",
    "                    changed = False\n",
    "                    # itterate non-bound elements\n",
    "                    for idx1 in self.first_heuristic_two():\n",
    "                        # h2-1\n",
    "                        idx2 = self.second_heuristic_one(idx1)\n",
    "                        if self.optimize_two(idx1, idx2):\n",
    "                            changed = True\n",
    "                            continue\n",
    "                        # h2-2,3 together if h2-1 not worked\n",
    "                        # concat idxs of heuristics in sequence\n",
    "                        extra_heuristics = np.concatenate([\n",
    "                            self.second_heuristic_two(idx1),\n",
    "                            self.second_heuristic_three(idx1)\n",
    "                        ])\n",
    "                        for idx2 in extra_heuristics:\n",
    "                            if self.optimize_two(idx1, idx2):\n",
    "                                changed = True\n",
    "                                break\n",
    "                    steps += 1\n",
    "        # after train return support vectors with lambdas and Y\n",
    "        mask = self.l > 0.0\n",
    "        return self.X[mask], self.Y[mask], self.l[mask], self.w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод опорных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return x1.dot(x2.T)\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma):\n",
    "    if len(x1.shape) == 1:\n",
    "        x1r = x1.reshape((1, x1.shape[0]))\n",
    "    else:\n",
    "        x1r = x1\n",
    "    if len(x2.shape) == 1:\n",
    "        x2r = x2.reshape((1, x2.shape[0]))\n",
    "    else:\n",
    "        x2r = x2\n",
    "    ans = np.zeros((x1r.shape[0], x2r.shape[0]))\n",
    "    for i in np.arange(x1r.shape[0]):\n",
    "        # \n",
    "        ans[i] = np.exp(-gamma  * ((x2r - x1r[i])**2).T.sum(axis = 0))\n",
    "    if len(x1.shape) == 1:\n",
    "        ans = ans[0]\n",
    "    if len(x2.shape) == 1:\n",
    "        ans = ans.T[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySVM:\n",
    "    def __init__(self, kernel = None, C=1.0, eps = 0.001, maxsteps=1000):\n",
    "        self.C = C\n",
    "        self.linear = False\n",
    "        if kernel is None:\n",
    "            kernel = lambda x1, x2: x1.dot(x2.T)\n",
    "            self.linear = True\n",
    "        self.K = kernel\n",
    "        self.w = None\n",
    "        self.w0 = None\n",
    "        self.l = None\n",
    "        self.svX = None\n",
    "        self.svY = None\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "    \n",
    "    def weights(self):\n",
    "        if self.linear:\n",
    "            return np.append(self.w0, self.w)\n",
    "        \n",
    "    def predict(self, X_val, border = 0):\n",
    "        # make predict: 0 - negative, 1 - positive\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        if self.linear:\n",
    "            x0 = np.ones((X_val.shape[0], 1))\n",
    "            X = np.hstack((x0, X_val))\n",
    "            # <w, x> product for all examples\n",
    "            Xw = X.dot(np.append(-self.w0, self.w))\n",
    "            # a(x) = [<w,x> > t], t - border\n",
    "            Y_pred[Xw >= border] = 1\n",
    "            return Y_pred\n",
    "        else:\n",
    "            yl = (self.svY * self.l).reshape((self.svY.shape[0], 1))\n",
    "            U = self.K(yl * self.svX, X_val).sum(axis = 0) - self.w0\n",
    "            Y_pred[U >= border] = 1\n",
    "            return Y_pred\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        X = X_train\n",
    "        Y = np.array(Y_train)\n",
    "        Y[Y_train == 0] = -1\n",
    "        smo = Binary_SMO(X, Y, self.K, self.C, self.eps, self.maxsteps, self.linear)\n",
    "        self.svX, self.svY, self.l, self.w0 = smo.train()\n",
    "        if self.linear:\n",
    "            self.w = smo.get_weights()\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Support Vector Machine\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SVM\"\n",
    "    \n",
    "    \n",
    "    # returns support vectors with lambdas\n",
    "    def vectors(self):\n",
    "        return self.svX, self.l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNode:\n",
    "    def __init__(self, idxs=None, pos=None, neg=None, c=None):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.positives = pos\n",
    "        self.negatives = neg\n",
    "        self.c = c\n",
    "        self.idxs = idxs\n",
    "\n",
    "    # setters:\n",
    "    def set_left(self, left_node):\n",
    "        self.left = left_node\n",
    "\n",
    "    def set_idxs(self, idxs):\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def set_right(self, right_node):\n",
    "        self.right = right_node\n",
    "\n",
    "    def set_predicat(self, predicat):\n",
    "        self.predicat = predicat\n",
    "\n",
    "    def set_class(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def set_positives(self, positives):\n",
    "        self.positives = positives\n",
    "        \n",
    "    def set_negatives(self, negatives):\n",
    "        self.negatives = negatives\n",
    "\n",
    "    #getters:\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "    def get_class(self):\n",
    "        return self.c\n",
    "\n",
    "    def get_idxs(self):\n",
    "        return self.idxs\n",
    "    \n",
    "    def get_positives(self):\n",
    "        return self.positives\n",
    "        \n",
    "    def get_negatives(self):\n",
    "        return self.negatives\n",
    "    \n",
    "    def get_len(self):\n",
    "        return self.idxs.shape[0]\n",
    "\n",
    "    #checkers:\n",
    "    def is_leaf(self):\n",
    "        return self.predicat is None\n",
    "\n",
    "    def is_inner(self):\n",
    "        return not self.is_leaf()\n",
    "    \n",
    "    def make_leaf(self):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bingini(*args):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Gini = 2 * p(+) * (1 - p(+))\n",
    "    if len(args) == 2:\n",
    "        p = args[0] / (args[1]+args[0])\n",
    "    else:\n",
    "        p = args[0].sum() / args[0].shape[0]\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def binentropy(*args):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Entropy = -p(+)log(p(+)) - (1 - p(+))log(1 - p(+))\n",
    "    if len(args) == 2:\n",
    "        p = args[0] / (args[1]+args[0])\n",
    "    else:\n",
    "        p = args[0].sum() / args[0].shape[0]\n",
    "    return -p*np.log(p) - (1 - p)*np.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDescisionTree:\n",
    "    def __init__(self, criteria=bingini, pruning_cost=None, min_samples_split=2, random_sub_num=None):\n",
    "        self.CATEGORICAL_LEN = 10\n",
    "        self.is_categorical = None\n",
    "        self.categorical_vals = {}\n",
    "        self.H = criteria\n",
    "        self.root = None\n",
    "        self.min_split = min_samples_split\n",
    "        if random_sub_num is None:\n",
    "            self.random_subspace = False\n",
    "        else:\n",
    "            self.random_subspace = True\n",
    "            self.feature_num = random_sub_num\n",
    "        if pruning_cost is None:\n",
    "            self.pruning = False\n",
    "        else:\n",
    "            self.pruning = True\n",
    "            self.alpha = pruning_cost\n",
    "            \n",
    "    def node_classify(self, node):\n",
    "        # set class num node:\n",
    "        positives = node.get_positives()\n",
    "        negatives = node.get_negatives()\n",
    "        if positives >= negatives:\n",
    "            node.set_class(1)\n",
    "        else:\n",
    "            node.set_class(0)\n",
    "            \n",
    "    # create root and recursive building tree\n",
    "    def build_tree(self):\n",
    "        # create root with all nums\n",
    "        self.root = BinaryNode(np.arange(self.Y.shape[0]))\n",
    "        self.root.set_positives(self.Y.sum())\n",
    "        negatives = self.root.get_len() - self.root.get_positives()\n",
    "        self.root.set_negatives(negatives)\n",
    "        # recursive function of creation\n",
    "        self.recursive_creation(self.root)\n",
    "            \n",
    "            \n",
    "    # may be modifed\n",
    "    def stop_criteria(self, node):\n",
    "        # if num of eelems in node less than min required for split => 1\n",
    "        if node.get_len() < self.min_split:\n",
    "            return True\n",
    "        # if all elems in node has only one class => 1\n",
    "        positives = node.get_positives()\n",
    "        negatives = node.get_negatives()\n",
    "        return (negatives==0 or positives==0)\n",
    "    \n",
    "    # union of search_best_split() and split_node()\n",
    "    def search_best_split(self, node):\n",
    "        X_iter = self.X[node.get_idxs()]\n",
    "        Y_iter = self.Y[node.get_idxs()]\n",
    "        # compute node info criteria\n",
    "        positiv = node.get_positives()\n",
    "        negativ = node.get_negatives()\n",
    "        node_info = self.H(positiv, negativ)\n",
    "        # best params\n",
    "        best_gain = 0.0\n",
    "        best_j, best_t = 0, 0.0\n",
    "        # search in all features:\n",
    "        if self.random_subspace:\n",
    "            # get random permutation\n",
    "            features = np.random.permutation(self.X.shape[1])\n",
    "            # stay only feature_num random features\n",
    "            features = features[:self.feature_num]\n",
    "        # else search by all features\n",
    "        else:\n",
    "            features = range(self.X.shape[1])\n",
    "\n",
    "        for j in features:\n",
    "            column = X_iter[:, j]\n",
    "            # fast search if categorical:\n",
    "            if self.is_categorical[j]:\n",
    "                possible_vals = self.categorical_vals[j]\n",
    "                for i in range(1, possible_vals.shape[0]):\n",
    "                    mask = column < possible_vals[i]\n",
    "                    Y_r = Y_iter[mask]\n",
    "                    if Y_r.shape[0] == 0 or Y_r.shape[0] == Y_iter.shape[0]:\n",
    "                        continue\n",
    "                    right_pos = Y_r.sum()\n",
    "                    right_neg = Y_r.shape[0] - right_pos\n",
    "                    right_gini = self.H(right_pos, right_neg)\n",
    "                    left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
    "                    # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "                    gain = node_info\n",
    "                    gain -= (Y_r.shape[0]*right_gini/node.get_len())\n",
    "                    gain -= (1 - Y_r.shape[0]/node.get_len())*left_gini\n",
    "                    if gain > best_gain:\n",
    "                        best_t = possible_vals[i]\n",
    "                        best_j = j\n",
    "                        best_gain = gain  \n",
    "                continue\n",
    "            # else standart search:\n",
    "            sorted_col = np.argsort(column)\n",
    "            right_neg = 0\n",
    "            right_pos = 0\n",
    "            last_t = column[sorted_col[0]]\n",
    "            for i in range(1, column.shape[0]):\n",
    "                if Y_iter[sorted_col[i-1]]:\n",
    "                    right_pos += 1\n",
    "                else:\n",
    "                    right_neg += 1\n",
    "                    \n",
    "                idx = sorted_col[i]\n",
    "                if column[idx] == last_t:\n",
    "                    continue\n",
    "                \n",
    "                last_t = column[idx]\n",
    "                # compute gain:\n",
    "                right_gini = self.H(right_pos, right_neg)\n",
    "                left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
    "                # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "                gain = node_info\n",
    "                gain -= (i*right_gini/node.get_len()) + (1 - i/node.get_len())*left_gini\n",
    "                # needs best gain split\n",
    "                if gain > best_gain:\n",
    "                    best_t = column[idx]\n",
    "                    best_j = j\n",
    "                    best_gain = gain\n",
    "                    \n",
    "        if best_gain > 0.0:\n",
    "            return best_j, best_t\n",
    "    \n",
    "    # create 2 new nodes: left and right\n",
    "    def split_node(self, node, j, t):\n",
    "        # set predicat rule for node\n",
    "        predicat = lambda x: x[j] < t\n",
    "        node.set_predicat(predicat)\n",
    "        # get split mask\n",
    "        column = self.X[node.get_idxs(), j]\n",
    "        mask = column < t\n",
    "        # make idxs for left and right\n",
    "        right_idxs = node.get_idxs()[mask]\n",
    "        left_idxs = node.get_idxs()[np.logical_not(mask)]\n",
    "        #compute positives:\n",
    "        right_pos = self.Y[right_idxs].sum()\n",
    "        left_pos = self.Y[left_idxs].sum()\n",
    "        # compute negatives\n",
    "        right_neg = right_idxs.shape[0] - right_pos\n",
    "        left_neg = left_idxs.shape[0] - left_pos\n",
    "        # create nodes\n",
    "        node.set_left(BinaryNode(left_idxs, left_pos, left_neg))\n",
    "        node.set_right(BinaryNode(right_idxs, right_pos, right_neg))\n",
    "    \n",
    "    # recursive function for nodes:\n",
    "    def recursive_creation(self, node):\n",
    "        # classify another node:\n",
    "        self.node_classify(node)\n",
    "        # stop criteria for building\n",
    "        if self.stop_criteria(node):\n",
    "            return\n",
    "        #else find best split\n",
    "        jt = self.search_best_split(node)\n",
    "        # if we cant find best split - stop\n",
    "        if jt is None:\n",
    "            return\n",
    "\n",
    "        # split node for 2 child:\n",
    "        self.split_node(node, *jt)\n",
    "        # start recursion for left:\n",
    "        self.recursive_creation(node.get_left())\n",
    "        # for right:\n",
    "        self.recursive_creation(node.get_right())\n",
    "    \n",
    "    def tree_pruning(self, node):\n",
    "        # this node R_a(t) computing:\n",
    "        # R = sum([y != c]) / |N|\n",
    "        if node.get_class():\n",
    "            R_a = node.get_negatives() / node.get_len()\n",
    "        else:\n",
    "            R_a = node.get_positives() / node.get_len()\n",
    "        # R_a(t) = R(t) + a\n",
    "        R_a += self.alpha\n",
    "        # at first go while not leaf:\n",
    "        if node.is_leaf():\n",
    "            # return R_a(leaf)\n",
    "            return R_a\n",
    "        # R_a(Tl) and R_a(Tr)\n",
    "        R_al = self.tree_pruning(node.get_left())\n",
    "        R_ar = self.tree_pruning(node.get_right())\n",
    "        # if R_a(t) < R_a(T) => pruning\n",
    "        if R_a <= R_al + R_ar:\n",
    "            node.make_leaf()\n",
    "            return R_a\n",
    "        # else do nothing\n",
    "        return R_al + R_ar\n",
    "    \n",
    "    def search_categorical(self):\n",
    "        self.is_categorical = np.zeros(self.X.shape[1]).astype(np.int8)\n",
    "        for j in range(self.X.shape[1]):\n",
    "            uniq = np.unique(self.X[:, j])\n",
    "            if uniq.shape[0] < self.CATEGORICAL_LEN:\n",
    "                self.categorical_vals[j] = uniq\n",
    "                self.is_categorical[j] = 1\n",
    "            \n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # temp sets for comfort\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # we will store only idxs while training in nodes =>\n",
    "        # make idxs column for comfort:\n",
    "        self.idxs = np.arange(X_train.shape[0])\n",
    "        # for speed search categorical\n",
    "        self.search_categorical()\n",
    "        # build tree \n",
    "        self.build_tree()\n",
    "        # pruning tree\n",
    "        if self.pruning:\n",
    "            self.tree_pruning(self.root)\n",
    "        #  delete temp sets:\n",
    "        #self.free_memory(root)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        del self.idxs\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(X_val.shape[0]):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while node.is_inner():\n",
    "            if node.predicat(x):\n",
    "                node = node.get_right()\n",
    "            else:\n",
    "                node = node.get_left()\n",
    "        # if in leaf:\n",
    "        return node.get_class()\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Decision Tree\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод k-ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkovski(x1, x2, p = 3):\n",
    "    return (np.abs(x1 - x2) ** p).T.sum(axis = 0)**(1.0 / p)\n",
    "\n",
    "def euclid(x1, x2):\n",
    "    return minkovski(x1, x2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biquadratical_kernel(t):\n",
    "    return (-t**2 + 1)**2\n",
    "\n",
    "def triquadratical_kernel(t):\n",
    "    return (-t**2 + 1)**3\n",
    "    \n",
    "def triqubical_kernel(t):\n",
    "    return (-t**3 + 1)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryKNN:\n",
    "    def __init__(self, k, metric = euclid, parsen_kernel = None):\n",
    "        self.k = k\n",
    "        self.p = metric\n",
    "        # algo shoud remember all data\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        if parsen_kernel is None:\n",
    "            # all elems have equal weight\n",
    "            self.Kp = lambda t: 1.0\n",
    "        else:\n",
    "            # parsen method\n",
    "            self.Kp = parsen_kernel\n",
    "\n",
    "    def stolp_filtration(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # just remmember data:\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # check correct of k\n",
    "        self.k = min(self.k, len(self.Y) - 1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(len(X_val)).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(len(X_val)):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        # compute all distances\n",
    "        r_x = self.p(self.X, x)\n",
    "        # take sorted order of dists in idxs\n",
    "        order = np.argsort(r_x)\n",
    "        # width for parsen is distance to k+1 neiborh:\n",
    "        h = r_x[order[self.k]]\n",
    "        # idxs of first k elems neibrh\n",
    "        order = order[:self.k]\n",
    "        # take first k Y:\n",
    "        Y_k = self.Y[order]\n",
    "        # compute parsen function for all neiborh:\n",
    "        K = self.Kp(r_x[order] / h)\n",
    "        \n",
    "        # compute functional for positives elems\n",
    "        pos_w = (K * Y_k).sum()\n",
    "        # compute functional for negatives elems\n",
    "        neg_w = (K * np.logical_not(Y_k)).sum()\n",
    "        \n",
    "        # class with more functional wins\n",
    "        return int(pos_w > neg_w) # 0 or 1\n",
    "\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"k Nearest Neighbor\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"KNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryRandomForest:\n",
    "    def __init__(self, Ntrees=20, criteria=bingini, random_sub_num=None):\n",
    "        self.N = Ntrees\n",
    "        self.criteria = criteria\n",
    "        self.sub_space_num = random_sub_num\n",
    "        # list for trained models\n",
    "        self.trees = [None for _ in range(self.N)]\n",
    "\n",
    "    # get random sample for tree training\n",
    "    def bootstrap_sample(self, X, Y):\n",
    "        # indexes of X and Y\n",
    "        indexes = np.arange(len(Y))\n",
    "        # rundom indexes with repeats\n",
    "        indexes = np.random.choice(indexes, len(indexes))\n",
    "        # bootstrap sample\n",
    "        return X[indexes], Y[indexes]\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        if self.sub_space_num is None:\n",
    "            self.sub_space_num = int(len(X_train)**(1/2))\n",
    "        # train N trees with \n",
    "        for i in range(self.N):\n",
    "            # create tree(use our class of tree)\n",
    "            self.trees[i] = BinaryDescisionTree(\n",
    "                self.criteria, # user criteria\n",
    "                None, # trees without prunning\n",
    "                2, # build while it possible \n",
    "                self.sub_space_num) # num of random features in random space method\n",
    "            # and train tree:\n",
    "            self.trees[i].fit(*self.bootstrap_sample(X_train, Y_train))\n",
    "        return self\n",
    "            \n",
    "\n",
    "    def predict(self, X_val):\n",
    "        voices = np.zeros(len(X_val))\n",
    "        # make vote:\n",
    "        for tree in self.trees:\n",
    "            voices += tree.predict(X_val)\n",
    "        # compute winners:\n",
    "        Y_pred = voices >= (self.N + 1) // 2\n",
    "        return Y_pred.astype(np.int8)\n",
    "\n",
    "    \n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Random Forest\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"RF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn реализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>mnth_3</th>\n",
       "      <th>mnth_4</th>\n",
       "      <th>mnth_5</th>\n",
       "      <th>mnth_6</th>\n",
       "      <th>mnth_7</th>\n",
       "      <th>mnth_8</th>\n",
       "      <th>mnth_9</th>\n",
       "      <th>mnth_10</th>\n",
       "      <th>mnth_11</th>\n",
       "      <th>mnth_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>9.35</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>9.35</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>9.35</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
       "0     13.4     22.9       0.6          5.2      9.35           44.0   \n",
       "1      7.4     25.1       0.0          5.2      9.35           44.0   \n",
       "2     12.9     25.7       0.0          5.2      9.35           46.0   \n",
       "\n",
       "   WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  ...  mnth_3  mnth_4  \\\n",
       "0          20.0          24.0         71.0         22.0  ...       0       0   \n",
       "1           4.0          22.0         44.0         25.0  ...       0       0   \n",
       "2          19.0          26.0         38.0         30.0  ...       0       0   \n",
       "\n",
       "   mnth_5  mnth_6  mnth_7  mnth_8  mnth_9  mnth_10  mnth_11  mnth_12  \n",
       "0       0       0       0       0       0        0        0        1  \n",
       "1       0       0       0       0       0        0        0        1  \n",
       "2       0       0       0       0       0        0        0        1  \n",
       "\n",
       "[3 rows x 130 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "test_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные с 2007 по 2015 год.\n"
     ]
    }
   ],
   "source": [
    "year_validate = train_df['Year'].to_numpy()\n",
    "print(\"Данные с\", year_validate.min(), \"по\", year_validate.max(), \"год.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим целевую переменную и нормализуем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_df['RainTomorrow'].to_numpy()\n",
    "Y_test = test_df['RainTomorrow'].to_numpy()\n",
    "\n",
    "X_train = train_df.drop('RainTomorrow', axis = 1)\n",
    "X_test = test_df.drop('RainTomorrow', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_features_std = MinMaxScaler()\n",
    "\n",
    "X_train = scale_features_std.fit_transform(X_train.to_numpy()) \n",
    "X_test = scale_features_std.transform(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кросс валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross_validation(model_from_param, param_generator):\n",
    "    max_val = 2015\n",
    "    min_val = 2013\n",
    "    best_score = 0.0\n",
    "    best_param = None\n",
    "    for p in tqdm(param_generator):\n",
    "        mean_score = 0.0\n",
    "        for year in range(min_val, max_val + 1):\n",
    "            mask_train = year_validate < year\n",
    "            mask_val = year_validate == year\n",
    "            X_t = X_train[mask_train]\n",
    "            Y_t = Y_train[mask_train]\n",
    "            X_v = X_train[mask_val]\n",
    "            Y_v = Y_train[mask_val]\n",
    "            model = model_from_param(p)\n",
    "            model.fit(X_t, Y_t)\n",
    "            mean_score += model.score(X_v, Y_v)\n",
    "        mean_score /= max_val - min_val + 1\n",
    "        if best_score < mean_score:\n",
    "            best_score = mean_score\n",
    "            best_param = p\n",
    "    return best_param, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:40:52<00:00, 605.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 0.30000000000000004 with accuracy: 0.8057910510872309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.01, 0.1)\n",
    "modeling = lambda c: BinaryLogisticRegression(reg_type='l2', C=c, maxsteps=1500, speed=0.02)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best param is\", best_C, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logistic Regression"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_log_l2_model = BinaryLogisticRegression(reg_type='l2', C=best_C, maxsteps=1500, speed=0.02)\n",
    "my_log_l2_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8151228151228151"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_log_l2_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логическая регрессия из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:42<00:00,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 0.6 with accuracy: 0.854795589212209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.01, 0.1)\n",
    "modeling = lambda c: LogisticRegression(penalty='l2', C=c, max_iter=250)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best param is\", best_C, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.6, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=250, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_log_l2_model = LogisticRegression(penalty='l2', C=best_C, max_iter=250)\n",
    "skl_log_l2_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421498421498421"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_log_l2_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающее дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_a = 0.000019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_desc_tree_model = BinaryDescisionTree(pruning_cost=best_a)\n",
    "my_desc_tree_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8183568183568184"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_desc_tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решающее дерево из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_desc_tree_model = DecisionTreeClassifier()\n",
    "skl_desc_tree_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7714252714252714"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_desc_tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод k-ближайших соседей из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:02:28<00:00, 1469.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 31 with accuracy: 0.813676096672508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_k = np.arange(1, 65, 15)\n",
    "modeling = lambda K: KNeighborsClassifier(n_neighbors=K)\n",
    "best_k, best_score = time_cross_validation(modeling, generate_k)\n",
    "print(\"Best param is\", best_k, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=31, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "skl_knn_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7934087934087934"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_knn_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя реализация KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNN"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn_model = BinaryKNN(k=best_k)\n",
    "my_knn_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7934087934087934"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод опорных векторов из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:04<00:00, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 0.6 with accuracy: 0.8544884970944056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.1, 0.1)\n",
    "modeling = lambda c: LinearSVC(C=c)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best param is\", best_C, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.6, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_svm_model = LinearSVC(C=best_C)\n",
    "skl_svm_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8414183414183414"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_svm_model.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
